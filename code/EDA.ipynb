{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Import neccessary support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2, os\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# load VGG-16 model: 23 layers, 138,357,544 params, 528MB\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# These models expect \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, list, '2377385', str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Image ID's from text file\n",
    "with open('../data/dog_data_part2.txt', 'r') as imagehandles:\n",
    "    # Store Image ID's as variable for image pre-processing, removing the endline character in the process\n",
    "    dog_data_part2 = [image_id[:-1] for image_id in imagehandles]\n",
    "# Check that all is copacetic\n",
    "len(dog_data_part2), type(dog_data_part2), dog_data_part2[0], type(dog_data_part2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate VGG-16 Model\n",
    "> * Image data format set in Keras config file at  '~/.keras/keras.json'\n",
    "{\n",
    "    \"floatx\": \"float32\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"image_data_format\": \"channels_last\" # \"Height-Width-Depth\" TensorFlow data format convention\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Vizualize filters and feature maps of CNNs with VGG-16\n",
    "# load VGG-16 model: 23 layers, 138,357,544 params, 528MB\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# Adjust input size of the model for include_top=False\n",
    "# new_input = Input(shape=(224,224,3)) --> MUST INCLUDE THIS PARAM AFTER CLASSIFICATION\n",
    "\n",
    "# load the model weights into memory\n",
    "# https://keras.io/api/applications/vgg/#vgg16-function\n",
    "model_vgg16 = VGG16(\n",
    "                include_top=True,   # include_top=False to load model wihtout the fully-connected output layers used to make predictions\n",
    "                weights=\"imagenet\", # Weights are downloaded automatically when instantiating a model: Keras Applications ~/.keras/models/\n",
    "                input_tensor=None,  # input_tensor = new_input = Input(shape=(224,224,3))\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                classifier_activation=\"softmax\",\n",
    "            )\n",
    "# Summarize the loaded model\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Summary\tKernel Size: ( K x K )\t\n",
      "\n",
      "Number\t\tName\t\t(K, K, Depth, Nodes)\n",
      "Layer 1:\tblock1_conv1\t(3, 3, 3, 64)\n",
      "Layer 2:\tblock1_conv2\t(3, 3, 64, 64)\n",
      "Layer 4:\tblock2_conv1\t(3, 3, 64, 128)\n",
      "Layer 5:\tblock2_conv2\t(3, 3, 128, 128)\n",
      "Layer 7:\tblock3_conv1\t(3, 3, 128, 256)\n",
      "Layer 8:\tblock3_conv2\t(3, 3, 256, 256)\n",
      "Layer 9:\tblock3_conv3\t(3, 3, 256, 256)\n",
      "Layer 11:\tblock4_conv1\t(3, 3, 256, 512)\n",
      "Layer 12:\tblock4_conv2\t(3, 3, 512, 512)\n",
      "Layer 13:\tblock4_conv3\t(3, 3, 512, 512)\n",
      "Layer 15:\tblock5_conv1\t(3, 3, 512, 512)\n",
      "Layer 16:\tblock5_conv2\t(3, 3, 512, 512)\n",
      "Layer 17:\tblock5_conv3\t(3, 3, 512, 512)\n",
      "Layer 20:\tfc1\t(25088, 4096)\n",
      "Layer 21:\tfc2\t(4096, 4096)\n",
      "\n",
      "Layer 5: (3, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# VGG-16 Layers with non-zero parameters\n",
    "layers_VGG16 = [1, 2, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16, 17, 20, 21, 22]\n",
    "\n",
    "# summarize filter shapes\n",
    "print('Layer Summary\\tKernel Size: ( K x K )\\t')\n",
    "print()\n",
    "print('Number\\t\\tName\\t\\t(K, K, Depth, Nodes)')\n",
    "layer_VGG16 = 0\n",
    "# Loop thru each layer in ImageNet Model to summarize\n",
    "for layer in model_vgg16.layers:\n",
    "    # check for convolutional layer\n",
    "    if ('conv' not in layer.name) & ('fc' not in layer.name):\n",
    "        layer_VGG16 += 1\n",
    "        continue # Exclude fully connected layers & return to loop beginning\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(f'Layer {layer_VGG16}:\\t{layer.name}\\t{filters.shape}')\n",
    "    layer_VGG16 += 1\n",
    "print()\n",
    "\n",
    "# Select layer from VGG-16 layers list above to retrieve weights\n",
    "layer_number = layers_VGG16[3]\n",
    "# Retrieve current weights from selected hidden layer: 'layer_number'\n",
    "# Weights rep state of layer, get_weights loads state into similarly parameterized layers\n",
    "filters_extract, biases_extract = model_vgg16.layers[layer_number].get_weights()\n",
    "print(f'Layer {layer_number}: {filters_extract.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64,), numpy.ndarray)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[0][0][0].shape, type(filters[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 64), numpy.ndarray)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[0][0].shape, type(filters[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3, 64), numpy.ndarray)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[0].shape, type(filters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3, 3, 64), numpy.ndarray)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters.shape, type(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t FILTER #\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAETCAYAAACWbduDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASxklEQVR4nO3df5BV9XnH8fclICsLjF2gaC2wNJpYfzbMTmdSW2w0YxMxGZhmUn/VxKnj1FCNjSGVhM6k05rWaabWVpukIREssVMnaTB1MFMMmoQO6iAQhcRi1OWHLmEXEFmX/QGe/nGWerPu4nPv3nu/e+++XzOM7PW59zzLc89nzzn37DmFLMuQpBQmpG5A0vhlAElKxgCSlIwBJCkZA0hSMgaQpGQMIEnJGECSkjGAJCVjAElKxgCSlIwBJCkZA0hSMgaQpGQMIEnJGECSkjGAJCVjAElKxgCSlIwBJCkZA0hSMgaQpGQMIEnJGECSkplY7hNnzpyZtba2hmqPHz8eqtuzZ0+orru7m97e3kKoWCUrZbb9/f2huo6OjlDdkSNHnG2VlDLXY8eOhepeeeWVUN1Icy07gFpbW9m8eXOo9tChQ6G622+/PVT3ve99L1Sn8pQy271794bqvvjFL4bqHn744VCdSlfKXLu6ukJ1y5cvD9WtXbt22MfdBZOUjAEkKRkDSFIykQD6JrAf2F7lXlRbzrVx1c1sIwG0CvhQlftQ7a3CuTaqVdTJbCMB9CPgYLUbUc0518ZVN7P1GJCkZAwgScmUfSJiT08PW7duDdU+8sgjobr777+/3HZUQT09PWzZsiVUG53tmjVrQnXRM6tVup6eHrZt2xaqXbduXahu1apVobqRzqx2C0hSMpEA+ndgE/BeYC/wJ1XtSLXiXBtX3cw2sgt2ddW7UArOtXHVzWzdBZOUjAEkKRkDSFIyBpCkZAwgSckYQJKSKftM6M7OTu67775Q7Q9+8INQ3aRJk0J10evVqjxdXV2sXLkyVBudbV9f32haUgV0dXXxta99LVT72GOPhepGuy66BSQpGQNIUjIGkKRkDCBJyRhAkpIxgCQlYwBJSsYAkpSMASQpGQNIUjKFLMvKe2Kh0Ansqmw7YfOyLJuVaNkNz9k2prE417IDSJJGy10wSckYQJKSMYAkJWMASUqm7AuSFQqF8NHrM888M1T32muvher6+voYGBgoRJev0pQy27lz54bqordcPnz4MD09Pc62Ck499dRs+vTpodqJE2PRcMYZZ4Tq2tvb6erqettcyw6gUixdujRU9/DDD4fqtm/fPpp2VEF33HFHqG7Xrtinv6tXrx5NOzqJ6dOnc+2114ZqW1paQnUrVqwI1bW1tQ37uLtgkpIxgCQlYwBJSsYAkpRMJIDmAI8DPwN2AJ+uakeqFefamOpqrpFPwY4BtwNbgGnAM8D6ajalmhhprj9N2ZRGra7mGtkC6iD/ZgCOkCdr7MQejWXOtTHV1VxLPQbUCrwPeKryrSihVpxrI2pljM+1lBMRpwLfAW4DXm9paWHRokWhJy5fvjxUF700yJ49e0J1CvmluQKzm5ubueCCC0JPvvnmm0N199xzT6guentuvaOhc2X//v3cfffdoSd/5CMfCdXddNNNobqRTkSNbgFNIv9mvgX8Z/A5GvuGzrUArE3akSqhbtbXSAAVgG+Q70v+Q3XbUQ0NN9dLgdgvbWmsqqv1NbILdjHwx8BzwLbBxz5ftY5UK8PN9UnyA5gLUzWlURtpfV2XrKOTiATQRvJUVWMZbq63AvMT9KLKqav11TOhVWwHsCB1Exo/DCAV2wA0pW5C44cBpGIZsDh1Exo/DCAN1ZG6AY0fBpCkZMq+JOsbb7zBpk2bQrVr1qwJ1a1fH/sd19dffz1Up/IUCoXwNYF/+MMfhup+8YtfhOoGBgZCdSrd/PnzufPOO0O10TlcccUVobqnn3562MfdApKUjAEkKRkDSFIyBpCkZAwgSckYQJKSMYAkJWMASUrGAJKUjAEkKZlC9ELwb3tiodAJDH+l6eqbl2XZrETLbnjOtjGNxbmWHUCSNFrugklKxgCSlIwBJCkZA0hSMmVfkKy5uTlraWkJ1fb3x+5119nZGarLsowsy+rm1iP1ZurUqdmMGTNCtX19faG6/fv3h+qcbfWMxbmWHUAtLS185jOfCdVG7+X+la98JVQX/cdReWbMmMEXvvCFUO2LL74YqoveGz76w0qlmzFjBsuXLw/VRud67733hupGWmfdBZOUjAEkKRkDSFIyBpCkZCIB1AQ8DfyE/N7hf1XVjlQrzrUx1dVcI5+C9QGXAt3AJGAj8Gg1m1JNjDTXJ1M2pVGrq7lGtoAy8m8G8m9o0uBjqm/OtTHV1Vyjx4DeBWwD9gPrgaeq1pFqybk2prqZa6mX4zgN+C5wS1tb23ObN28OPenYsWOhurPOOitU19HRQV9fn2fLVs7/zxXobGtr2xedbfQ22RdddFGo7tVXX3W2lVM81+1tbW1ZdK7d3d3vXAScf/75obqR1tlSPwV7DXgC+FCJz9PYVjzXtWlbUQWN+fU1EkCzyJMU4FTgg8DzVetItTLcXCcD/i5Efaur9TXyKdgZwGry/coJwEPAI9VsSjUx3FyPAFuAhQn70ujU1foaCaBngfdVuxHV3HBzvTVFI6qoulpfPRNaxXYAC1I3ofHDAFKxDeRn0ko1YQCpWAYsTt2Exg8DSEN1pG5A44cBJCmZsi/J+sorr7BixYpQbfSsyl27Ut20UcX27dvHXXfdFart6uoK1bW3t4+iI1VCR0cHX/rSl0K10bmOdp11C0hSMgaQpGQMIEnJGECSkjGAJCVjAElKxgCSlIwBJCkZA0hSMgaQpGRKvSj9W08sFDqBVL87MS/LslmJlt3wnG1jGotzLTuAJGm03AWTlIwBJCkZA0hSMgaQpGQMIEnJlH1FxJkzZ2atra2h2oGBgVDdhAmxPNy9ezcHDhzw/uFVUsps+/tjN1ItFGLj2rt3r7OtkmrMNbrO7tmzZ9i5lh1Ara2tRG9039ERu855U1PsjjCXXnppqE7lKWW2u3fvDtVNnjw5VHf55ZeH6lS6Uua6d+/eUN2UKVNCdSOts+6CSUrGAJKUjAEkKZlSAuhdwFbgkSr1ojSca2Oqi7mWEkCfBn5WrUaUjHNtTHUx12gA/TqwCFhZxV5Ue861MdXNXKMB9I/A54A3q9iLas+5Nqa6mWskgK4E9gPPVLkX1dZIc52doBdVTl2tr5ETES8GPgpcATQB04E1hw4d4qGHHgot5MYbbwzVLV26NFS3b9++UJ1Oati5Au8+ePAga9asCb3IsmXLQnVXX311qM7ZjtpIc73u4MGDPPjgg6EXue2220J1N9xwQ6hupLlGtoCWk+9TtgJXARuA60JL1Vg23FzvB2Ln4Gusqqv11fOAVOw8YEvqJjR+lBpAT5DvY6qxPIFzbURPMMbn6haQiu0AFqRuQuOHAaRiG8gPXEo1YQCpWAYsTt2Exg8DSEPFLt4kVYABJCkZA0hSMmVfkrWvr4+XXnopVHvkyJFQXfT1+vr6QnUqT29vLzt37gzVRs9cjr6es62e3t5eXnjhhVBtZ2dnqG60c3ULSFIyBpCkZAwgSckYQJKSMYAkJWMASUrGAJKUjAEkKRkDSFIyBpCkZApZlpX3xEKhE9hV2XbC5mVZNivRshues21MY3GuZQeQJI2Wu2CSkjGAJCVjAElKxgCSlEzZFyRramrKmpubQ7Xz588P1b355puhul27dnHgwIFCqFgla2pqyqZNmxaqnTdvXkWX3d7eTldXl7Otgqampmzq1Kmh2tbW1lBddJ3dvXv3sHMtO4Cam5v58Ic/HKqN3mc8euXESy65JFSn8kybNo3Fi2M3x/j6179e0WW3tbVV9PX0lqlTp7Jo0aJQ7erVq0N1R48eDdVdfPHFwz7uLpikZAwgSckYQJKSMYAkJRM9CN0OHAGOA8cAjxQ2hnacayNqp07mWsqnYB8AuqrViJJxro2pLubqLpikZKIBlAH/DTwD3FS9dlRjw811drp2VCF1s75Gd8EuBl4FfhVYDzzf3d3Nxo0bQ0/u7e2taF307Eu9o7fNFbiru7ubJ598MvQChw8fDtVNmBD7WedsK2K4uf6op6eHbdu2hV7g+PHjobrRXs4nugX06uB/9wPfBX57VEvVWDF0rtcA/enaUYXUzfoaCaBmYFrR3y8HtletI9XKcHPNgC3JOlIl1NX6GtkFm02eoifqHwS+X7WOVCvDzfUIEPvNYY1VdbW+RgLoJeCiajeimhturpcBf5igF1VOXa2vfgyvYhuAptRNaPwwgFQsA2LX4ZAqwADSUB2pG9D4YQBJSsYAkpRM2Zdk7e/vZ9eu2E0WL7vsslDd7Nmx3wKILlfl6e3tZfv22KkjH//4x0N1c+fODdXt2bMnVKfSHT16lGeffTZUu2TJklDd6aefHqobaa5uAUlKxgCSlIwBJCkZA0hSMgaQpGQMIEnJGECSkjGAJCVjAElKxgCSlEyh3ItKFwqFTiDV70TMy7JsVqJlNzxn25jG4lzLDiBJGi13wSQlYwBJSsYAkpSMASQpmbIvSDZx4sRs8uTJodopU6aE6kq5hfPAwEAhVKyStbS0ZHPmzAnVvvzyy6G697znPaG69vZ2urq6nG0VzJgxo+JzPfvss0N1I8217ACaPHky55xzTqi2ra0tVLdz585Q3ebNm0N1Ks+cOXN49NFHQ7XXXXddqG7Dhg2huuh7RaWbM2cOjz32WKj2+uuvD9WtW7cuVDfSXN0Fk5SMASQpGQNIUjIGkKRkogF0GvBt4HngZ8D7q9aRasm5Nqa6mWv0U7B7gO8DHwNOAWKfq2usc66NqW7mGgmg6cBC4JODX/cP/lF9c66Nqa7mGtkF+w2gE7gf2AqsBJqr2ZRqwrk2prqaa2QLaCKwALgFeIp88+6O2bNns2zZstBCrrrqqlBd9KS26O1ldVLDzhW498CBAzzwwAOhF3n88cdDdVdeeWWo7uc//3moTiMaaa5/2dXVxcqVK0MvEj0R9ZprrgnVjXRmdWQLaO/gn6cGv/42+Teo+jbSXNcm60iVUFfraySA9gF7gPcOfn0Z8NOqdaRaGW6u3Yzh4wUKqav1Nfop2C3At8iPqL8E3AB8tlpNqWaGznUTcDr5QUzVr+HW1zEpGkDbAH9LsPEMneutqRpRRdXN+uqZ0Cq2gzF8vECNxwBSsQ1AU+omNH4YQCqWAYtTN6HxwwDSUB2pG9D4YQBJSqbsGxOecsop2cyZM0O10ctAnnfeeeHlZ1nmdYOr5MILL8yil9rct29fqO60004L1S1ZsoTnnnvO2VbBggULso0bN4Zqt2/fHqo799xzQ3ULFy5ky5Ytb5urW0CSkjGAJCVjAElKxgCSlIwBJCkZA0hSMgaQpGQMIEnJGECSkjGAJCVT9q9iFAqFTmBXZdsJm5dl2axEy254zrYxjcW5lh1AkjRa7oJJSsYAkpSMASQpGQNIUjIGkKRkDCBJyRhAkpIxgCQlYwBJSsYAkpSMASQpGQNIUjIGkKRkDCBJyVQygI4D24CfAFuA3xl8vBUYep/X3wceGfLYKuBjg39/AvjfwdfbBny7gn3q5GYDDwIvAc8Am4Al5DM7DGwFnge+XPScTwKdvDWvbcC55LM/OuTx6wef0w58p+g1Pkb+HlDtnVh3twP/BZy4j3YrJ5/fc4N/fgr8DTC51AVPLL/ntzkK/Nbg3/8A+FvgklG83rXA5tE2pZIUgLXAauCawcfmAR8FDgE/Bq4ETiUPou8C/zNY9x/Anw15vVbgRd56XwzVBpwH7KhI9ypX8bq7GlgK3Dn49cnm9wGgC5gK/Ovgn0+UsuBq7YJNJ3/Dqr5cCvQDXy16bBfwz0PqTvxUPHOUy/sy8PlRvoYqaxOlz7Ub+FNgMdBSyhMruQV0Kvmbsgk4g/zNPBrfIn+jA6wHlo3y9fTOziPffX4nvwKcDfyo6LE/An636Ov3D/733eTvixNuId+SAngI+BRwVjnNquLeBVwGfKPosZPNr9jrwMvk74unogus1i7Y+4EHgPNHqB3pOrDFj7sLlt595KHST/4D4PeAZ4H3An8H7CuqHW4XDE6+CX8c+HtgOfBoZVpWGU5sPLSSH/dbX/T/Tja/oQqlLrhau2CbgJnASBcXP0D+U7RYC/n+pNLZASwo+nop+U/EE3P8MXAhcAFwM/E35sn8G7AQmFuB11J5Tmw8zANOIZ97qaaRB9jOUp5UrQA6h3xz7sAI//8F4NeA3xz8eh5wEb+8qafa20C+C31z0WNThqnbSf4hw19UYJkDwN3AbRV4LY3OYeBW4LPApBKeNxX4F/IPMEo69luNY0CQb4p9gnwTG/JN9r1FtX8OXAfcT/6GHwBuJP8HOKH4GFAX8MEK9qrhZeQHEu8GPkf+0fobDB80XyV/o84f/HroMaBPAa/y9mMI3wT+achrfQNYMcreVRlbyU+luYp8i/dk83ucfF2fQP6J6F+XujBvyyMpGc+ElpSMASQpGQNIUjIGkKRkDCBJyRhAkpIxgCQlYwBJSub/AIvJMPqL+sm7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix, fc='grey')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            c = '\\nBLUE'\n",
    "        elif j == 1:\n",
    "            c = '\\nGREEN'\n",
    "        else:\n",
    "            c ='\\nRED'\n",
    "        plt.xlabel(xlabel=c,color='w')\n",
    "        plt.ylabel(ylabel=f'  \\n\\n{i+1}',color='w',rotation=0)\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "print('\\t\\t FILTER #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_data_part2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../../visual_genome_part2/VG_100K_2/2405343.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "dog_x = image.img_to_array(img)\n",
    "dog_x = np.expand_dims(dog_x, axis=0)\n",
    "dog_x = preprocess_input(dog_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "\n",
      "\t37.11%\t:keeshond\n",
      "\t13.47%\t:schipperke\n",
      "\t8.45%\t:Newfoundland\n",
      "\t7.30%\t:groenendael\n",
      "\t6.35%\t:Norwegian_elkhound\n"
     ]
    }
   ],
   "source": [
    "# Extract features from image_id = dog_x with VGG-16\n",
    "preds = model_vgg16.predict(dog_x)\n",
    "\n",
    "# Decode extracted features into class, description, and probability\n",
    "\n",
    "preds_class_breed_score = decode_predictions(preds, top=5)[0]\n",
    "print('Prediction: \\n')\n",
    "for i in range(len(preds_class_breed_score)):\n",
    "    print(f'\\t{preds_class_breed_score[i][2]*100:.2f}%\\t:{preds_class_breed_score[i][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n02112350', 'keeshond', 0.37105247),\n",
       " ('n02104365', 'schipperke', 0.13470888),\n",
       " ('n02111277', 'Newfoundland', 0.0845376),\n",
       " ('n02105056', 'groenendael', 0.073017746),\n",
       " ('n02091467', 'Norwegian_elkhound', 0.06352974),\n",
       " ('n02500267', 'indri', 0.03845513),\n",
       " ('n02445715', 'skunk', 0.035748463),\n",
       " ('n02108551', 'Tibetan_mastiff', 0.032140084),\n",
       " ('n02106662', 'German_shepherd', 0.01633126),\n",
       " ('n02106382', 'Bouvier_des_Flandres', 0.010834407),\n",
       " ('n02106166', 'Border_collie', 0.009284958),\n",
       " ('n02110063', 'malamute', 0.00925284),\n",
       " ('n02110185', 'Siberian_husky', 0.0068449895),\n",
       " ('n03404251', 'fur_coat', 0.006372816),\n",
       " ('n02099267', 'flat-coated_retriever', 0.004811044),\n",
       " ('n02112137', 'chow', 0.0047492017),\n",
       " ('n02105412', 'kelpie', 0.004737555),\n",
       " ('n02112018', 'Pomeranian', 0.0045249825),\n",
       " ('n02111129', 'Leonberg', 0.00441722),\n",
       " ('n02109961', 'Eskimo_dog', 0.004194549),\n",
       " ('n02114367', 'timber_wolf', 0.0029230756),\n",
       " ('n02133161', 'American_black_bear', 0.0027088977),\n",
       " ('n02817516', 'bearskin', 0.0024210245),\n",
       " ('n02437616', 'llama', 0.0019322414),\n",
       " ('n02092002', 'Scottish_deerhound', 0.0018413556)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(preds, top=25)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 400, 3) <class 'numpy.ndarray'>\n",
      "(500, 400, 3) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# NOTICE CODE FROM\n",
    "# https://thispointer.com/python-how-to-get-the-list-of-all-files-in-a-zip-archive/\n",
    "img_path = '../../visual_genome_part2/VG_100K_2/2405343.jpg'\n",
    "\n",
    "# Save image in set directory \n",
    "# Load RGB image \n",
    "img = cv2.imread(img_path)\n",
    "print( img.shape, type(img))\n",
    "# Read image binary data of 'VG_100K_2' from zip archive('visual_genome_part2.zip')\n",
    "#in_bytes = z.read(ith_image)\n",
    "# Decode bytes to image\n",
    "#img = cv2.imdecode(np.frombuffer(in_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "img = preprocess_input(img)\n",
    "print( img.shape, type(img))\n",
    "# Output img with window name as 'image' \n",
    "cv2.imshow('img', img)\n",
    "# Display for 1sec = 1_000ms\n",
    "#cv2.waitKey(1000)\n",
    "\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE CODE FROM\n",
    "# https://thispointer.com/python-how-to-get-the-list-of-all-files-in-a-zip-archive/\n",
    "\n",
    "# ~5.5 GB ZIP Archive, 40% of total data set\n",
    "with ZipFile('../../visual_genome_part2.zip', \"r\") as z:\n",
    "    # One file in zip archive\n",
    "    VG_100K_2 = z.namelist()\n",
    "\n",
    "    # Iterate over image file names, 'VG_100K_2/image_id.jpg'\n",
    "    for ith_image in VG_100K_2: \n",
    "        # Get extension of file, '.jpg'\n",
    "        ext = os.path.splitext(ith_image)[-1]\n",
    "        # Get root of file, root = VG_100K_2/image_id.jpg\n",
    "        root = os.path.splitext(ith_image)[0]\n",
    "        \n",
    "        # Skip over Archive Directory\n",
    "        if (ext == \".jpg\"):\n",
    "            # Skip root[:10]='VG_100K_2' in dog_pic_ids\n",
    "            if int(root[10:]) == 2405343:\n",
    "                # Read image binary data of 'VG_100K_2' from zip archive('visual_genome_part2.zip')\n",
    "                in_bytes = z.read(ith_image) # VG_100K_2/\n",
    "                # Decode bytes to image\n",
    "                img = cv2.imdecode(np.frombuffer(in_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "    \n",
    "                # Output img with window name as 'image' \n",
    "                cv2.imshow('img', img)\n",
    "                # Display for 1sec = 1_000ms\n",
    "                cv2.waitKey(30*1000)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#model.save('/../assets/model.h5')\n",
    "\n",
    "print(\"Saved model to disk\")\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 400, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26000/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000*52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
