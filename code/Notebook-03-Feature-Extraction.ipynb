{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 03: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "# load VGG-16 model: 23 layers, 138,357,544 params, 528MB\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# load and evaluate a saved model \n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "\n",
    "from keras import models\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten #, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Please install GPU version of TF\n",
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "# Sometimes my tensorflow tries to use GPU support but I don't want it to, have had many errors.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "sess_cpu = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(device_count={'GPU': 0}))\n",
    "print(tf.__version__)\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading binary X_train from pkl...\n",
      "Loading binary y_train from pkl...\n",
      "Training data loaded!\n",
      "\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load training data from previous notebooks\n",
    "print('Loading binary X_train from pkl...')\n",
    "# Save X_train and y_train arrays as pkl files in assets folder (just outside of Github repo) to load in Notebook 3\n",
    "with open('../../assets/X_training_image_data.pkl', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "    \n",
    "print('Loading binary y_train from pkl...')\n",
    "with open('../../assets/y_training_labels.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "print('Training data loaded!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading binary X_valid from pkl...\n",
      "Loading binary y_valid from pkl...\n",
      "Validation data loaded!\n",
      "\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load validation data from previous notebooks\n",
    "print('Loading binary X_valid from pkl...')\n",
    "# Save X_valid and y_valid arrays as pkl files in assets folder (just outside of Github repo) to load in Notebook 3\n",
    "with open('../../assets/X_validation_image_data.pkl', 'rb') as file:\n",
    "    X_valid = pickle.load(file)\n",
    "    \n",
    "print('Loading binary y_valid from pkl...')\n",
    "with open('../../assets/y_validation_labels.pkl', 'rb') as file:\n",
    "    y_valid = pickle.load(file)\n",
    "print('Validation data loaded!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Validation data: 43,733\n",
      "Length of Validation labels: 43,733\n",
      "\n",
      "Length of Training data: 64,346\n",
      "Length of Training labels: 64,346\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of Validation data: {len(X_valid):,}\\nLength of Validation labels: {len(y_valid):,}')\n",
    "print()\n",
    "print(f'Length of Training data: {len(X_train):,}\\nLength of Training labels: {len(y_train):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape: (224, 224, 3)\n",
      "Training data shape:   (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation data shape: {X_valid[0].shape}\\nTraining data shape:   {X_train[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List objects must be converted to numpy arrays before feeding tranfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_valid2 = np.array(X_valid)\n",
    "print('Loaded!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train2 = np.array(X_train)\n",
    "print('Loaded!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "y_valid2 = np.array(y_valid)\n",
    "print('Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "y_train2 = np.array(y_train)\n",
    "print('Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_valid2[0].shape)\n",
    "print(type(X_valid2[0]))\n",
    "print(type(X_valid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train2[0].shape)\n",
    "print(type(X_train2[0]))\n",
    "print(type(X_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/api/applications/vgg/#vgg16-function\n",
    "# https://towardsdatascience.com/a-demonstration-of-transfer-learning-of-vgg-convolutional-neural-network-pre-trained-model-with-c9f5b8b1ab0a\n",
    "\n",
    "# Adjust input size of the model for include_top=False\n",
    "new_input = Input(shape=(224,224,3)) \n",
    "\n",
    "# load the model weights into memory\n",
    "# Cut-Off the VGG-16 Model after the last Conv2D layer (18)\n",
    "base_model = VGG16(\n",
    "                include_top=False,   # include_top=False to load model wihtout the fully-connected output layers used to make predictions\n",
    "                weights=\"imagenet\", # Weights are downloaded automatically when instantiating a model: Keras Applications ~/.keras/models/\n",
    "                input_tensor=new_input, # --> MUST INCLUDE THIS PARAM TO FEED CLASSIFIER VGG-16 WEIGHTS\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                classifier_activation=\"softmax\",\n",
    "            )\n",
    "\n",
    "# Freeze Conv2D layers from training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Summarize the loaded model after dropping the dense top layers for binary classification\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8192)              205529088 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              16779264  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 241,221,441\n",
      "Trainable params: 226,506,753\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add dense layers to base_model loaded\n",
    "transfer_model = Sequential()\n",
    "transfer_model.add(base_model)\n",
    "\n",
    "# Flatten layer to transform 4D --> 2D for \"fully-connected\" dense layers\n",
    "transfer_model.add(Flatten())\n",
    "# Hiddden dense layer with 8,192 nodes\n",
    "transfer_model.add(Dense(8192, activation='relu'))\n",
    "transfer_model.add(Dropout(0.3))\n",
    "# Hidden dense layer with 2,048 nodes\n",
    "transfer_model.add(Dense(2048, activation='relu'))\n",
    "transfer_model.add(Dropout(0.1))\n",
    "# Hidden dense layer with 2,048 nodes\n",
    "transfer_model.add(Dense(2048, activation='relu'))\n",
    "transfer_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Summarize the new binary classifier to check if all looks ok\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2**i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish update frequency and early stopping\n",
    "bs = 64 # Chose by BASE-2, and 128 was too large for memory (both CoLab's and mine) NEED BATCH GENERATOR!!! \n",
    "nb_epochs = 1\n",
    "\n",
    "# Compile the model before training\n",
    "transfer_model.compile(\n",
    "              # Optimization Algorithm, Extension of Stochastic Gradient Descent, \n",
    "              optimizer = 'adam', # lr varies from 1 to 0(perfect), optimizer=Adam(lr=args.learning_rate)\n",
    "              # Objective Function, determines what will be used to fit the model\n",
    "              loss = 'binary_crossentropy',\n",
    "              # Accuracy metric helps with interpretation of model performance\n",
    "              metrics = ['accuracy'])\n",
    "              #options = run_opts)\n",
    "\n",
    "# Save the weights at each epoch they improve the model's performance\n",
    "checkpoint_saver = ModelCheckpoint(filepath = '../../assets/transfer.h5', # why not .hdf5 file ext?\n",
    "                               verbose = 1,\n",
    "                               save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - ETA: 0s - loss: 0.9855 - accuracy: 0.9625"
     ]
    }
   ],
   "source": [
    "# Fit the transfer model (using 'Adam': extension of stochastic gradient descent (SGD))\n",
    "transfer_history = transfer_model.fit(x=X_train2, \n",
    "                                   y=y_train2,\n",
    "                                   validation_data=(X_valid2, y_valid2),\n",
    "                                   batch_size=bs,\n",
    "                                   epochs=nb_epochs,\n",
    "                                   callbacks = [checkpoint_saver],\n",
    "                                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future work: Batch Generator- GPU can't store entire dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "ImageDataGenerator("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Generator from car simulation was more complex than needed here\n",
    "model.fit_generator(batch_generator(args.data_dir, X_train, y_train, args.batch_size, True),\n",
    "model.fit_generator(batch_generator('data', X_train, y_train, 40, True),\n",
    "                        args.samples_per_epoch, # steps_per_epoch\n",
    "                        args.nb_epoch,  # epochs\n",
    "                        max_q_size=1,\n",
    "                        validation_data=batch_generator(args.data_dir, X_valid, y_valid, args.batch_size, False),\n",
    "                        validation_data=batch_generator('data', X_valid, y_valid, 40, False),\n",
    "                        nb_val_samples=len(X_valid), # validation_steps\n",
    "                        callbacks=[checkpoint], # callback objects can write logs to monitor metrics, save model to disk, early stopping, or view internal states/stats after each batch or epoch \n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "def batch_generator(data_dir, image_paths, steering_angles, batch_size, is_training):\n",
    "def batch_generator('data', image_paths, steering_angles, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Generate training image give image paths and associated steering angles\n",
    "    \"\"\"\n",
    "    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n",
    "    steers = np.empty(batch_size)\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # argumentation\n",
    "            if is_training and np.random.rand() < 0.6:\n",
    "                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                image = load_image(data_dir, center)\n",
    "            # add the image and steering angle to the batch\n",
    "            images[i] = preprocess(image)\n",
    "            steers[i] = steering_angle\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                break\n",
    "        yield images, steers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From General Assembly Notes Lesson 04 Week 07 Topic: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy scores of model after trained on data\n",
    "# CREDIT DIRECT FROM NOTES LESSON 7p04 CNN\n",
    "def plot_loss(history, model_name):\n",
    "    train_loss = history.history['loss'] \n",
    "    test_loss = history.history['val_loss'] \n",
    "    epoch_labels = history.epoch \n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(12, 8)) \n",
    "    # Generate line plot of training, testing loss over epochs\n",
    "    plt.plot(train_loss, label='Training Loss', color='#185fad') \n",
    "    plt.plot(test_loss, label='Validation Loss', color='orange') \n",
    "    # Set title\n",
    "    plt.title(f'Model: {model_name}\\nTrain & Validation Loss by Epoch', fontsize=25) \n",
    "    plt.xlabel('Epoch', fontsize=18) \n",
    "    plt.ylabel('Categorical Crossentropy', fontsize=18) \n",
    "    plt.xticks(epoch_labels, epoch_labels)\n",
    "    plt.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT DIRECT FROM NOTES LESSON 7p04 CNN\n",
    "plot_loss(trans_history, 'Transfer from VGG-16 to Binary Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# load weights into new model\n",
    "loaded_model = '../../assets/model_vgg16_flatten.h5'\n",
    "\n",
    "# Load base model weights\n",
    "loaded_model = load_model(loaded_model)\n",
    "\n",
    "#loaded_model.load_weights(\"~\\.keras\\models.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Cut-Off the VGG-16 Model after the last Conv2D layer (18)\n",
    "dogg16 = models.Model(inputs=base_model.input,\n",
    "                           outputs=base_model.get_layer('flatten').output\n",
    "                          )\n",
    "'''\n",
    "# Extract features\n",
    "#flatten_features = model_vgg16.predict(x)\n",
    "# save model and architecture to single file\n",
    "#dogg16.save('../../model_dogg16.h5') \n",
    "#print(\"Saved model to disk\")\n",
    "'''\n",
    "transfer_model.save('../../assets/transfer_model.h5')\n",
    "print(\"Saved model to disk\")\n",
    "del transfer_model\n",
    "print(\"Transfer model deleted from memory!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "X_train                    -> '<unavailable>'\n",
      "X_valid                    -> '<unavailable>'\n",
      "dog_data_part2             -> ['2377385', '2377411', '2377429', '2377466', '2377\n",
      "y_valid                    -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.16.1\n"
     ]
    }
   ],
   "source": [
    "!ipython --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X_train and y_train arrays as pkl files in assets folder (just outside of Github repo) to load in Notebook 3\n",
    "print('Saving binary X_train to pkl...')\n",
    "# Save X_train and y_train arrays as pkl files in assets folder (just outside of Github repo) to load in Notebook 3\n",
    "with open('../../assets/X_training_image_data.pkl', 'wb') as outfile:\n",
    "    pickle.dump(X_train, outfile, pickle.DEFAULT_PROTOCOL)\n",
    "    \n",
    "print('Saving binary y_train to pkl...')\n",
    "with open('../../assets/y_training_labels.pkl', 'wb') as outfile:\n",
    "    pickle.dump(y_train, outfile, pickle.DEFAULT_PROTOCOL)\n",
    "print()\n",
    "print('Training data Pickled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "for env in os.listdir('/Users/me/miniconda3/envs'):\n",
    "    subprocess.call(['conda', 'list', '-n', env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(\n",
    "            obj,\n",
    "            file,\n",
    "            protocol=None,\n",
    "            *,\n",
    "            fix_imports=True,\n",
    "            buffer_callback=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.86-1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
