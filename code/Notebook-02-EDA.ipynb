{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 02: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2, os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#import matplotlib.image as mpimg\n",
    "#from tensorflow.keras.models import Sequential \n",
    "#from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# load VGG-16 model: 23 layers, 138,357,544 params, 528MB\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# These models expect \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, list, '2377385', str)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Image ID's from text file\n",
    "with open('../data/dog_data_part2.txt', 'r') as imagehandles:\n",
    "    # Store Image ID's as variable for image pre-processing, removing the endline character in the process\n",
    "    dog_data_part2 = [image_id[:-1] for image_id in imagehandles]\n",
    "# Check that all is copacetic\n",
    "len(dog_data_part2), type(dog_data_part2), dog_data_part2[0], type(dog_data_part2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate VGG-16 Model<br>\n",
    ">  Image data format: Set in Keras JSON config file<br>\n",
    "> - Stored at  '~/.keras/keras.json'<br>\n",
    "{<br>\n",
    "    \"floatx\": \"float32\",<br>\n",
    "    \"epsilon\": 1e-07,<br>\n",
    "    \"backend\": \"tensorflow\",<br>\n",
    "    \"image_data_format\": \"channels_last\"<br>\n",
    "}<br><br>\n",
    "- NOTE: \"Channels Last = (height, width, depth)-TensorFlow data format convention<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize filters and feature maps of CNNs with VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load VGG-16 model: 23 layers, 138,357,544 params, 528MB\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# Adjust input size of the model for include_top=False\n",
    "# new_input = Input(shape=(224,224,3)) --> MUST INCLUDE THIS PARAM AFTER CLASSIFICATION\n",
    "\n",
    "# load the model weights into memory\n",
    "# https://keras.io/api/applications/vgg/#vgg16-function\n",
    "model_vgg16 = VGG16(\n",
    "                include_top=True,   # include_top=False to load model wihtout the fully-connected output layers used to make predictions\n",
    "                weights=\"imagenet\", # Weights are downloaded automatically when instantiating a model: Keras Applications ~/.keras/models/\n",
    "                input_tensor=None,  # input_tensor = new_input = Input(shape=(224,224,3))\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                classifier_activation=\"softmax\",\n",
    "            )\n",
    "# Summarize the loaded model with all layers (include_top=True)\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Summary (by network layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Summary\tKernel Size: ( K x K )\t\n",
      "\n",
      "Number\t\tName\t\t(K, K, Depth, Nodes)\n",
      "Layer 1:\tblock1_conv1\t(3, 3, 3, 64)\n",
      "Layer 2:\tblock1_conv2\t(3, 3, 64, 64)\n",
      "Layer 4:\tblock2_conv1\t(3, 3, 64, 128)\n",
      "Layer 5:\tblock2_conv2\t(3, 3, 128, 128)\n",
      "Layer 7:\tblock3_conv1\t(3, 3, 128, 256)\n",
      "Layer 8:\tblock3_conv2\t(3, 3, 256, 256)\n",
      "Layer 9:\tblock3_conv3\t(3, 3, 256, 256)\n",
      "Layer 11:\tblock4_conv1\t(3, 3, 256, 512)\n",
      "Layer 12:\tblock4_conv2\t(3, 3, 512, 512)\n",
      "Layer 13:\tblock4_conv3\t(3, 3, 512, 512)\n",
      "Layer 15:\tblock5_conv1\t(3, 3, 512, 512)\n",
      "Layer 16:\tblock5_conv2\t(3, 3, 512, 512)\n",
      "Layer 17:\tblock5_conv3\t(3, 3, 512, 512)\n",
      "Layer 20:\tfc1\t(25088, 4096)\n",
      "Layer 21:\tfc2\t(4096, 4096)\n",
      "\n",
      "Layer 1: (3, 3, 3, 64)\n"
     ]
    }
   ],
   "source": [
    "# VGG-16 Layers with non-zero parameters\n",
    "layers_VGG16 = [ 1, 2, 4, 5, 7, 8, 9, 11, 12, 13, 15, 16, 17, 20, 21, 22 ]\n",
    "# Choose a layer to perform feature extraction\n",
    "layer_VGG16  = 0\n",
    "# summarize filter shapes\n",
    "print('Layer Summary\\tKernel Size: ( K x K )\\t')\n",
    "print()\n",
    "print('Number\\t\\tName\\t\\t(K, K, Depth, Nodes)')\n",
    "# Loop thru each layer in ImageNet Model to summarize\n",
    "for layer in model_vgg16.layers:\n",
    "    # check for convolutional layer\n",
    "    if ('conv' not in layer.name) & ('fc' not in layer.name):\n",
    "        layer_VGG16 += 1\n",
    "        continue # Exclude fully connected layers & return to loop beginning\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(f'Layer {layer_VGG16}:\\t{layer.name}\\t{filters.shape}')\n",
    "    layer_VGG16 += 1\n",
    "print()\n",
    "\n",
    "# Select layer from VGG-16 layers list above to retrieve weights\n",
    "layer_number = layers_VGG16[0]\n",
    "# Retrieve current weights from selected hidden layer: 'layer_number'\n",
    "# Weights rep state of layer, get_weights loads state into similarly parameterized layers\n",
    "filters_extract, biases_extract = model_vgg16.layers[layer_number].get_weights()\n",
    "print(f'Layer {layer_number}: {filters_extract.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3, 3, 64), numpy.ndarray)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (kernel-height, kernel-width, channel-depth, No. of filters)\n",
    "# 64-filters each (3 x 3 x 3) 3D volume, cubic filters\n",
    "filters_extract.shape, type(filters_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 64),\n",
       " array([[[0.2410788 , 0.14405024, 0.17188708],\n",
       "         [0.21905349, 0.13779737, 0.1578574 ],\n",
       "         [0.22949417, 0.12949093, 0.16341406]],\n",
       " \n",
       "        [[0.22994165, 0.14908592, 0.17978509],\n",
       "         [0.1863384 , 0.16121466, 0.18800756],\n",
       "         [0.11930275, 0.        , 0.04622722]],\n",
       " \n",
       "        [[0.22533065, 0.14236566, 0.17052189],\n",
       "         [0.20956579, 0.09347925, 0.15874107],\n",
       "         [0.22243272, 0.13306199, 0.17015679]]], dtype=float32))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters_extract[0][0].shape, filters_extract.min(3),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.8601116 , 0.9545779 , 0.89960235],\n",
       "        [0.915383  , 1.        , 0.96599036],\n",
       "        [0.8567038 , 0.97389966, 0.925328  ]],\n",
       "\n",
       "       [[0.810127  , 0.86608565, 0.84025717],\n",
       "        [0.90729994, 0.9373233 , 0.91953784],\n",
       "        [0.87189955, 0.9382962 , 0.90447164]],\n",
       "\n",
       "       [[0.779941  , 0.8438289 , 0.8172243 ],\n",
       "        [0.8836065 , 0.9460532 , 0.92962444],\n",
       "        [0.8408625 , 0.8799469 , 0.8426137 ]]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " filters_extract.max(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Second Layer (First Conv2D Layer immediately following the input layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLayer 1:  FILTER #\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAELCAYAAACxhLGjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPEUlEQVR4nO3dfZBddX3H8ffZh7tP2ZCHzcYkkruggoC0QAORBykqVaQ0o4I8FIUObRkFCrS2OEg7I2OrdqQKVhxxCg222MpI6xSm/EErKQ+NaXkITwF1tIRSsWQjzwQhyekfZ1PXuDfe/Z29d/d77vs1s5Pde+/3/L5zv3c/e869N+dmeZ4jSVF1zXYDklSGISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCq0ntTDrGciz2nCJlXvTa4GVKxaXqt/82EPjeZ4vKbWRispqXTn9yQ8N6OsutX596Why7db/2coLz7yYlWqgovqzLB8usd/Sn5W7W0dXLEqu3fzjFxh/aduUDaSHWG2Yvv1PTW6KhcvTa4HLPnVmqfpzjxzbXGoDVdbfA6vTg4R955da/k8uvjC59pMf+HSptatsmC5OZjC5fr/+Wqn1L7z4tOTa1Vd+veF1Hk5KCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYptGZC7DrgaeDhFvei9nKu1dVRs20mxNYCJ7S4D7XfWpxrVa2lg2bbzFks7gDGdr+we3CY4cOOTV74/e85KLkW4EOr6qXqzy1VXQlTzhWge16NBUel379nHX9Uci3AaW88I7n26r5rSq1dEVPOdmF3N+9bkH76rOPWlPud7T73E8m12Q13NrzO58QkhWaISQrNEJMUmiEmKbRmQuzvgPXA/sCTwG+3tCO1i3Otro6abTOvTqa/VKS5zLlWV0fN1sNJSaEZYpJCM8QkhWaISQrNEJMUmiEmKTRDTFJohpik0AwxSaE18479KS1fMsTHf/eI5IV/7Q1Lk2sBtu/YWapejS1fuIhLTjk1uf6ksfeUWr+nqze5NsuyUmtX2fDr9uKdF6xJru86udz/Xsq6k+MGaDxX98QkhWaISQrNEJMUmiEmKTRDTFJohpik0AwxSaEZYpJCM8QkhWaISQrNEJMUmiEmKTRDTFJohpik0AwxSaFleZ6nFWbZFmDzzLbTVvU8z5fMdhNzUfDZOtcGqjrX5BCTpLnAw0lJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmjJn2Y5MjKS1+tjyQtvfnZbci3A6FCtVP2jD20c902RUxsZGcnrYyuT63/w3H+XWn+gJ/3Dc5996llefvZlP0F3CiOD/Xl9r6H0DWx/rdT6T4y/lFz7Ajt5Jc+nnGtyiNXrY9y94Z7kps6/6aHkWoDzVtdL1a/aZ6+o71xuufrYSu7ecFdy/Rn/fFGp9Q9ZuiK59pqzv1Jq7Sqr7zXEhnNOSK7f+b9bSq1/4bXrk2tv4uWG13k4KSk0Q0xSaIaYpNAMMUmhGWKSQjPEJIVmiEkKzRCTFJohJik0Q0xSaIaYpNAMMUmhNRNiewO3A48CjwDl/nev5grnWk0dN9dmzmKxHfgocB8wDNwL3NbKptQWjea6aTabUmkdN9dmQuypiS+AFygSfsWPt73KjRvTzxv1tc9ck1wLkF364VL1mnquwKZt27ex6ZkHkjf8zS/cWqqxgY+uSa79yY7tpdaugIZzzZatpOeyLyVveMct15Zq7Itv3i+59j+vurHhddN9TmwMOBTYkNyN5qIxnGsVjdEBc51OiM0DbgIuBp5vTTuaBc61mjpmrs2GWC/FHXID8A+ta0dt5lyrqaPm2kyIZcC1FMfWn2ttO2oj51pNHTfXZkLsaOBDwDuAjRNfJ7ayKbWFc62mjptrM69O3kWR7qoW51pNHTdX37EvKTRDTFJohpik0AwxSaEZYpJCM8QkhWaISQrNEJMUmiEmKbRm3rE/pXm9PRy59+LkhY8464zkWoBzDl1Rqr7cmZGqLSdne55+Xq5933VAqfX3XZD+uKp1Jz+kK++17zzGj459a3J9rdZdav21G5/6xTdq4OlXnmt4nXtikkIzxCSFZohJCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmiGmKTQsjzP0wqzbAuweWbbaat6nudLZruJuSj4bJ1rA1Wda3KISdJc4OGkpNAMMUmhGWKSQjPEJIVmiEkKzRCTFJohJim05E8a7RtekA+NLE9eeKC33Adxjs7rK1W/8f57x31T5NRq8/vzwdF5yfVDveVms3RwNLn2ic1PMD6+NSvVQEUtrvXk9YH02WSD5eaavW7v5NrHn3iS8a1TzzU5xIZGlvPuy29IburAZcPJtQAXHrNPqfoFgz1R37nccoOj8zjmL9Yk1x+xfKzU+n9wyEXJtce+9bhSa1dZfaCPfzsy/dPZBw59U6n1uz92ZXLt4W9/V8PrPJyUFJohJik0Q0xSaIaYpNAMMUmhGWKSQjPEJIVmiEkKzRCTFJohJik0Q0xSaIaYpNCaCbF+4D+AB4BHgMtb2pHaxblWU8fNtZmPbMuAIeBFoBe4C7ioa3B0fd/+p6av3FNLrwU23fqpUvX7Lhm4N8/zVaU2EtuUcwW+nc2v5axOPx0Oi8qdsuXJ6/41ufbEt63hgfse7ORT8TSc65KsOz+ZweQNL+1NPukNAJ/43h3JtYf/xunc8+AjU861mT2xnOIOgeJO6Z24TLE512rquLk2+5xYN7AReBq4DdjQso7UTs61mjpqrs2G2A7gEOD1wBHAW1rWkdrJuVZTR811uq9OPgusA06Y+VY0i5xrNXXEXJsJsSXAgonvB4Djgcda1pHaxblWU8fNtZmXG5YB11McZ3cBNwK3tLIptYVzraaOm2szIfYgcGirG1HbOddq6ri5+o59SaEZYpJCM8QkhWaISQrNEJMUmiEmKTRDTFJohpik0JJPEDS6fJQzL78geeHFQ+XOTbRsQX+pejU2smwh77/0lOT6lfNHSq2/uD/9XGY9XeUeV1W2cvEQV61ZnVzfvazEOeaAbMnK9OI9nH/QPTFJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmhZnudphVm2Bdg8s+20VT3P8yWz3cRcFHy2zrWBqs41OcQkaS7wcFJSaIaYpNAMMUmhGWKSQjPEJIVmiEkKzRCTFJohJim05I9LznoG8qw2nL5yb8lP8M53lCt/4YfjvrN7almtK6e/xCdp93eXa2BniTdgv/ga+Ss7snINVFN/luXDJfZbhrrK3a3bS4z1mXwnL+U7p2wgPcRqw/Ttf2p6V8v2S68FeOXFcuW3Xxb1v1+0Xn8PrC7xkfX7LSi3/rbt6bX/9Hi5tStsmC5OZjC5ftVgX6n1n9m+M7n2qp883/A6DyclhWaISQrNEJMUmiEmKTRDTFJohpik0AwxSaEZYpJCM8QkhWaISQrNEJMUmiEmKbTphFg3cD9wS4t60exwrtXUMXOdzlksLgIeBeYDDC5ayMGnfyB54VsuODq5FuCz675fqv7Pb7+sVH2F/MxcAeaNDHPIOccmb/Cm915RqqHrNq1Nrv3it68utXaF/Nxclw/WuPzAvZM3OHrzzaUa2vG5S5Nr//6r/9Lwumb3xF4P/DrwV8ldaC5yrtXUUXNtNsSuBC4B0k8IpLnIuVZTR821mRA7CXgauLfFvai9nGs1ddxcm3lO7GhgDXAi0E9xjP23rWxKbdForh+czaZUWsfNtZk9sUspjrHHgNOBb1HhO6SDONdq6ri5+j4xSaFN94NC1k18qVrW4VyraB0dMFf3xCSFZohJCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkKb7ptd/19/rYcD6ouSFx7qS14agF9ZMVyqXo3Nq9U4ul5Prp9fW1Bq/cOWHpxcO9gzWGrtKusZqrHo8LHk+mz+SKn1s0NXpdd+498bXueemKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYpNENMUmiGmKTQDDFJoRlikkIzxCSFZohJCs0QkxSaISYptCzP87TCLNsCbJ7Zdtqqnuf5ktluYi4KPlvn2kBV55ocYpI0F3g4KSk0Q0xSaIaYpNAMMUmhGWKSQjPEJIVmiEkKzRCTFJohJik0Q0xSaIaYpNAMMUmhGWKSQjPEJIXWzhDbAWwEHgDuA46auHwMeHi32x4H3LLbZWuBUya+Xwd8Z2J7G4FvzHCvamwp8DXgB8C9wHrgfRQzew64H3gMuGJSzW8BW/jpvDYCB1LMfttul581UfM4cNOkbZxC8RhQ++363X0YuBlYMHH5GHue30MTX5uAPwX6WtFcTys22sA24JCJ798NfBr41RLbOxO4p2xTmpYM+CZwPfCbE5fVgTXAM8CdwEnAAEWY/SNw98Ttvg5csNv2xoDv89PHxe5WAQcBj8xI90o1+Xf3euB84M8mft7T/N4OjAPzgK9MfJ09083N1uHkfIoHvWJ5B/Aq8OVJl20G/nK32+3667yi5HpXAB8vuQ3NrPVMf64vAh8G3gssmumG2rknNkDxwO4HllH8QpRxA8UvC8BtwB+V3J5+sYMongr4RRYCbwLumHTZacAxk34+cuLfN1A8Lnb5PYo9OoAbgfOAN6Y0qxnXDbwTuHbSZXua32TPA/9F8bjYMJNNzdbh5JHAV4G3NLhto3NmT77cw8nZdzVFML1K8UfkbcCDwP7AZ4AfTbrtVIeTsOfDkR3AZ4FLgVtnpmUl2LUDMkbxPOhtk67b0/x2l81sW4XZOpxcD4wAjT7QYSvFX/PJFlEcX2v2PAIcNunn8yn+Mu+a453ALwEHAx+h+Qf3nvwNcCywcga2pTS7dkDqQI1i7tM1TBGC3525tgqzFWJvptg13drg+u8By4EDJn6uA7/Mz+62qv2+RfF0wEcmXTY4xe2+S/HCzcdmYM3XgM8DF8/AtlTOc8CFwB8CvdOomwd8ieJFoRl/Lnw2nhODYrfybIrDBSgOP56cdNvfBz4I/DXFL81rwO9Q3Im7TH5ObBw4viVda7Kc4snZzwOXULxt4iWmDqsvUzzY95n4effnxM4DfsjPP6dyHfCF3bZ1LfDHJXvXzLif4m1Sp1Psee9pfrdT/K53UbxS/clWNORHtkkKzXfsSwrNEJMUmiEmKTRDTFJohpik0AwxSaEZYpJCM8QkhfZ/1nHHEpQYhDIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters_extract.min(), filters_extract.max()\n",
    "# Sets the minimum to zero and the maximum to one\n",
    "filters_extract = (filters_extract - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 4, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters_extract[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        if j == 0:\n",
    "            c = '\\nBLUE'\n",
    "            color_map = 'Blues'\n",
    "        elif j == 1:\n",
    "            c = '\\nGREEN'\n",
    "            color_map = 'Greens'\n",
    "        else:\n",
    "            c ='\\nRED'\n",
    "            color_map = 'Reds'\n",
    "        ax = plt.subplot(n_filters, 3, ix, fc='grey')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.xlabel(xlabel=c,color='w')\n",
    "        plt.ylabel(ylabel=f'  \\n\\n{i+1}',color='w',rotation=0)\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap=color_map)\n",
    "        ix += 1\n",
    "#print('\\t\\t FILTER #')\n",
    "print(f'\\tLayer {layer_number}:  FILTER #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Visual Genome Dog Data, chosen at random, on VGG-16 Loaded Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: (for random dog = 2382269) \n",
      "\n",
      "\t45.91%\t:marimba\n",
      "\t9.28%\t:stage\n",
      "\t8.05%\t:container_ship\n",
      "\t4.88%\t:palace\n",
      "\t4.32%\t:panpipe\n"
     ]
    }
   ],
   "source": [
    "# NOTICE, SOME CODE IN THIS CELL FROM:\n",
    "# https://thispointer.com/python-how-to-get-the-list-of-all-files-in-a-zip-archive/\n",
    "\n",
    "# Input shape acrandom_dogby VGG-16 = 224 x 224 x3\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Choose a dog randomly to test breed accuracy\n",
    "dog_pic_x = random.randint(0, len(dog_data_part2)-1)\n",
    "random_dog = int(dog_data_part2[dog_pic_x])\n",
    "\n",
    "# ~5.5 GB ZIP Archive, 40% of total data set\n",
    "# Open, read, and close ZIP file for faster image pre-processing\n",
    "with ZipFile('../../visual_genome_part2.zip', \"r\") as z:\n",
    "    # One file in zip archive\n",
    "    VG_100K_2 = z.namelist()\n",
    "\n",
    "    # Iterate over image file names, 'VG_100K_2/image_id.jpg'\n",
    "    for ith_image in VG_100K_2: \n",
    "        # Get extension of file, '.jpg'\n",
    "        ext = os.path.splitext(ith_image)[-1]\n",
    "        # Get root of file, root = VG_100K_2/image_id.jpg\n",
    "        root = os.path.splitext(ith_image)[0]\n",
    "        \n",
    "        # Skip over Archive Directory\n",
    "        if (ext == \".jpg\"):\n",
    "            \n",
    "            # Skip root[:10]='VG_100K_2' in dog_pic_ids\n",
    "            if int(root[10:]) == random_dog:\n",
    "                \n",
    "                # Read image binary data of 'VG_100K_2' from zip archive('visual_genome_part2.zip')\n",
    "                in_bytes = z.read(ith_image) # VG_100K_2/\n",
    "                # Decode bytes to image\n",
    "                img = cv2.imdecode(np.frombuffer(in_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "                # Input shape accepted by VGG-16 = 224 x 224 x3\n",
    "                img = cv2.resize(img, dsize=(img_width, img_height))\n",
    "                # Convert a PIL image instance to a Numpy array\n",
    "                dog_x = image.img_to_array(img)\n",
    "                # Expand the shape of the array: Insert new axis at 'axis' position in expanded array shape\n",
    "                dog_x = np.expand_dims(dog_x, axis=0)\n",
    "                # Returns array with type: 'float32', covert: RGB --> BGR, & zero-center each channel wrt ImageNet dataset\n",
    "                dog_x = preprocess_input(dog_x)\n",
    "                \n",
    "                # Extract features from image_id = dog_x with VGG-16\n",
    "                preds = model_vgg16.predict(dog_x)\n",
    "\n",
    "                # Decode extracted features into class, description, and probability\n",
    "                preds_class_breed_score = decode_predictions(preds, top=5)[0]\n",
    "                \n",
    "                # Display the prediction corresponding to the image\n",
    "                print(f'Prediction: (for random dog = {random_dog}) \\n')\n",
    "                for i in range(len(preds_class_breed_score)):\n",
    "                    print(f'\\t{preds_class_breed_score[i][2]*100:.2f}%\\t:{preds_class_breed_score[i][1]}')\n",
    "\n",
    "                # Output img with window name as 'image' \n",
    "                cv2.imshow('img', img)\n",
    "                \n",
    "                # Display for 30 secs = 1_000ms * 30\n",
    "                #cv2.waitKey(30*1000)\n",
    "                \n",
    "                # Display image indefinitely\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/../assets/model.h5')\n",
    "\n",
    "print(\"Saved model to disk\")\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "\n",
      "\t37.11%\t:keeshond\n",
      "\t13.47%\t:schipperke\n",
      "\t8.45%\t:Newfoundland\n",
      "\t7.30%\t:groenendael\n",
      "\t6.35%\t:Norwegian_elkhound\n"
     ]
    }
   ],
   "source": [
    "img_path = '../../visual_genome_part2/VG_100K_2/2405343.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "dog_x = image.img_to_array(img)\n",
    "dog_x = np.expand_dims(dog_x, axis=0)\n",
    "dog_x = preprocess_input(dog_x)\n",
    "\n",
    "# Extract features from image_id = dog_x with VGG-16\n",
    "preds = model_vgg16.predict(dog_x)\n",
    "\n",
    "# Decode extracted features into class, description, and probability\n",
    "\n",
    "preds_class_breed_score = decode_predictions(preds, top=5)[0]\n",
    "print('Prediction: \\n')\n",
    "for i in range(len(preds_class_breed_score)):\n",
    "    print(f'\\t{preds_class_breed_score[i][2]*100:.2f}%\\t:{preds_class_breed_score[i][1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
